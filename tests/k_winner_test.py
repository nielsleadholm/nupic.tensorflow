#  Numenta Platform for Intelligent Computing (NuPIC)
#  Copyright (C) 2019, Numenta, Inc.  Unless you have an agreement
#  with Numenta, Inc., for a separate license for this software code, the
#  following terms and conditions apply:
#
#  This program is free software: you can redistribute it and/or modify
#  it under the terms of the GNU Affero Public License version 3 as
#  published by the Free Software Foundation.
#
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
#  See the GNU Affero Public License for more details.
#
#  You should have received a copy of the GNU Affero Public License
#  along with this program.  If not, see http://www.gnu.org/licenses.
#
#  http://numenta.org/licenses/
#

import random
import unittest

import numpy as np
import tensorflow as tf
from tensorflow.python import keras
from tensorflow.python.keras import keras_parameterized, testing_utils
from tensorflow.python.platform import test
from tensorflow.python.training import gradient_descent

from nupic.tensorflow.constraints import SparseWeights
from nupic.tensorflow.layers import KWinners, KWinners2d
from nupic.tensorflow.layers.k_winners import compute_kwinners


SEED = 18
CUSTOM_OBJECTS = {
    "KWinners": KWinners,
    "KWinners2d": KWinners2d,
    "SparseWeights": SparseWeights,
}


class KWinnersTestBase(object):
    """
    Base module for setting up tests.
    """

    @classmethod
    def setUpClass(cls):
        tf.set_random_seed(SEED)
        np.random.seed(SEED)
        random.seed(SEED)

    def setUp(self):
        super(KWinnersTestBase, self).setUp()

        # Batch size 2
        x = np.random.random((2, 7)).astype(np.float32) / 2.0
        x[0, 1] = 1.20
        x[0, 2] = 1.10
        x[0, 3] = 1.30
        x[0, 5] = 1.50
        x[1, 0] = 1.11
        x[1, 2] = 1.21
        x[1, 4] = 1.31
        x[1, 6] = 1.22
        self.x1 = x

        # All equal duty cycle for x.
        self.duty_cycles1 = np.full(shape=(7,), fill_value=1.0 / 3.0, dtype=np.float32)

        # Batch size 2
        x2 = np.random.random((2, 6)).astype(np.float32) / 2.0
        x2[0, 0] = 1.50
        x2[0, 1] = 1.02
        x2[0, 2] = 1.10
        x2[0, 3] = 1.30
        x2[0, 5] = 1.03
        x2[1, 0] = 1.11
        x2[1, 1] = 1.04
        x2[1, 2] = 1.20
        x2[1, 3] = 1.60
        x2[1, 4] = 1.01
        x2[1, 5] = 1.05
        self.x2 = x2

        # Unequal duty cycle for x2.
        duty_cycle2 = np.zeros(6, dtype=np.float32)
        duty_cycle2[0] = 1.0 / 2.0
        duty_cycle2[1] = 1.0 / 4.0
        duty_cycle2[2] = 1.0 / 2.0
        duty_cycle2[3] = 1.0 / 4.0
        duty_cycle2[4] = 1.0 / 2.0
        duty_cycle2[5] = 1.0 / 4.0
        self.duty_cycles2 = duty_cycle2

        # Batch size 2, but with negative entries.
        x3 = np.random.random((2, 6)).astype(np.float32) - 0.5
        x3[0, 1] = -1.20
        x3[0, 2] = 1.20
        x3[0, 3] = 1.03
        x3[0, 5] = 1.01
        x3[1, 1] = 1.21
        x3[1, 2] = -1.21
        x3[1, 5] = 1.02
        self.x3 = x3

        # Unequal duty cycle for x3.
        duty_cycle3 = np.zeros(6, dtype=np.float32)
        duty_cycle3[1] = 0.001
        duty_cycle3[2] = 100
        self.duty_cycles3 = duty_cycle3

        # Batch size 1.
        x4 = np.random.random((1, 10)).astype(np.float32) / 2.0
        x4[0, 2] = 1.20
        x4[0, 3] = 1.21
        x4[0, 4] = 1.22
        x4[0, 5] = 1.23
        x4[0, 6] = 1.30
        x4[0, 7] = 1.31
        x4[0, 8] = 1.32
        x4[0, 9] = 1.33
        self.x4 = x4

        # All equal duty cycle for x4.
        self.duty_cycles4 = np.ones(10, dtype=np.float32) / 10


class KWinnersFowardTest(KWinnersTestBase, keras_parameterized.TestCase):
    """
    Module for testing the forward pass, i.e. the compute_kwinners function.
    """

    def test_one(self):
        """boost strength 0 to 10, k=3, batch size 2"""

        # Setup input.
        x = self.x1
        duty_cycles = self.duty_cycles1

        # Test forward pass through the layer.
        expected = np.zeros(x.shape, dtype=np.float32)
        expected[0, 1] = x[0, 1]
        expected[0, 3] = x[0, 3]
        expected[0, 5] = x[0, 5]
        expected[1, 2] = x[1, 2]
        expected[1, 4] = x[1, 4]
        expected[1, 6] = x[1, 6]

        # Loop over floating point boost strengths.
        for b in np.arange(0.0, 10.0, dtype=np.float32):
            # Build layer with varying boost_strength.
            result = compute_kwinners(x, 3, duty_cycles, boost_strength=b)
            self.assertAllEqual(result, expected)

    def test_two(self):
        """
        Unequal duty cycle, boost strength 0 to 10, k = 3, batch size 2.
        """

        # Setup input.
        x = self.x2
        duty_cycles = self.duty_cycles2

        # Test forward with boost strength of 0.
        expected = np.zeros(x.shape, dtype=np.float32)
        expected[0, 0] = x[0, 0]
        expected[0, 2] = x[0, 2]
        expected[0, 3] = x[0, 3]
        expected[1, 0] = x[1, 0]
        expected[1, 2] = x[1, 2]
        expected[1, 3] = x[1, 3]

        result = compute_kwinners(x, 3, duty_cycles, boost_strength=0.0)
        self.assertAllEqual(result, expected)

        # Test forward again with boost strength of 1.
        expected = np.zeros(x.shape, dtype=np.float32)
        expected[0, 0] = x[0, 0]
        expected[0, 5] = x[0, 5]
        expected[0, 3] = x[0, 3]
        expected[1, 1] = x[1, 1]
        expected[1, 3] = x[1, 3]
        expected[1, 5] = x[1, 5]

        result = compute_kwinners(x, 3, duty_cycles, boost_strength=1.0)
        self.assertAllEqual(result, expected)

        # Test forward again with boost strength from 2 to 10. Should give save result
        # given the differing duty cycles.
        expected = np.zeros(x.shape, dtype=np.float32)
        expected[0, 1] = x[0, 1]
        expected[0, 3] = x[0, 3]
        expected[0, 5] = x[0, 5]
        expected[1, 1] = x[1, 1]
        expected[1, 3] = x[1, 3]
        expected[1, 5] = x[1, 5]

        for b in np.arange(2.0, 10.0, dtype=np.float32):
            result = compute_kwinners(x, 3, duty_cycles, boost_strength=b)
            self.assertAllEqual(result, expected)

    def test_three(self):
        """
        Unequal duty cycle, boost factor 0 (and then over a range), k = 3, batch size 2.
        """

        # Setup input.
        x = self.x3
        duty_cycles = self.duty_cycles3

        # Test forward with boost factor of 0.
        expected = np.zeros(x.shape, dtype=np.float32)
        expected[0, 2] = x[0, 2]
        expected[0, 3] = x[0, 3]
        expected[1, 1] = x[1, 1]
        expected[1, 5] = x[1, 5]

        result = compute_kwinners(x, 2, duty_cycles, boost_strength=0.0)
        self.assertAllEqual(result, expected)

        # Test forward again with boost factor from 1 to 10. Should yield the same
        # result as the negative numbers will never be in the top k and the non-one
        # values have very large duty cycles.
        expected = np.zeros(x.shape, dtype=np.float32)
        expected[0, 3] = x[0, 3]
        expected[0, 5] = x[0, 5]
        expected[1, 1] = x[1, 1]
        expected[1, 5] = x[1, 5]

        for b in np.arange(2.0, 10.0, dtype=np.float32):
            result = compute_kwinners(x, 2, duty_cycles, boost_strength=b)
            self.assertAllEqual(result, expected)

    def test_four(self):
        """
        All equal duty cycle, boost factor 0, k = 0,1, and n, batch size 1.
        """

        # Setup input.
        x = self.x4
        duty_cycles = self.duty_cycles4

        # Test forward with boost factor of 1 and k=0.
        expected = np.zeros(x.shape, dtype=np.float32)

        result = compute_kwinners(x, 0, duty_cycles, boost_strength=1.0)
        self.assertAllEqual(result, expected)

        # Test forward with boost factor of 1 and k=1.
        expected[0, -1] = x[0, -1]

        result = compute_kwinners(x, 1, duty_cycles, boost_strength=1.0)
        self.assertAllEqual(result, expected)

        # Test forward with boost factor of 1 and k=1.
        expected = np.copy(x)

        result = compute_kwinners(x, 10, duty_cycles, boost_strength=1.0)
        self.assertAllEqual(result, expected)

    def test_tie_breaking(self):
        """
        Test k-winners with tie-breaking
        """
        x = self.x2
        # Force tie breaking
        x[0, 5] = x[0, 1]

        # Expected with [0, 1] winning the tie-break
        expected1 = np.zeros_like(x)
        expected1[0, 0] = x[0, 0]
        expected1[0, 1] = x[0, 1]
        expected1[0, 3] = x[0, 3]
        expected1[1, 1] = x[1, 1]
        expected1[1, 3] = x[1, 3]
        expected1[1, 5] = x[1, 5]

        # Expected with [0, 5] winning the tie-break
        expected2 = np.zeros_like(x)
        expected2[0, 0] = x[0, 0]
        expected2[0, 3] = x[0, 3]
        expected2[0, 5] = x[0, 5]
        expected2[1, 1] = x[1, 1]
        expected2[1, 3] = x[1, 3]
        expected2[1, 5] = x[1, 5]
        result = compute_kwinners(x, k=3, duty_cycles=self.duty_cycles2,
                                  boost_strength=1.0)
        result = keras.backend.get_value(result)
        self.assertTrue(np.array_equal(result, expected1) or
                        np.array_equal(result, expected2))


class KWinners1DLayerTest(KWinnersTestBase, keras_parameterized.TestCase):
    """
    Module for testing the 1D KWinners layer.
    """

    @keras_parameterized.run_all_keras_modes
    def test_one(self):
        # Set input, output, and layer params.
        x = self.x2
        expected = np.zeros(x.shape, dtype=np.float32)
        expected[0, 0] = x[0, 0]
        expected[0, 2] = x[0, 2]
        expected[0, 3] = x[0, 3]
        expected[1, 0] = x[1, 0]
        expected[1, 2] = x[1, 2]
        expected[1, 3] = x[1, 3]
        kwargs = {
            "percent_on": 0.333,
            "k_inference_factor": 1.5,
            "boost_strength": 1.0,
            "boost_strength_factor": 0.5,
            "duty_cycle_period": 1000,
        }

        # Use testing utils to validate layer functionality.
        with self.cached_session(), keras.utils.custom_object_scope(CUSTOM_OBJECTS):
            testing_utils.layer_test(KWinners,
                                     kwargs=kwargs,
                                     input_data=x,
                                     expected_output=expected,
                                     )

    @keras_parameterized.run_all_keras_modes
    def test_two(self):
        # Set input, output, and layer params.
        x = self.x2
        expected = np.zeros(x.shape, dtype=np.float32)
        expected[0, 0] = x[0, 0]
        expected[0, 2] = x[0, 2]
        expected[0, 3] = x[0, 3]
        expected[1, 0] = x[1, 0]
        expected[1, 2] = x[1, 2]
        expected[1, 3] = x[1, 3]

        # Test layer within Sequential model.
        with self.cached_session():
            # Compile model. Results should be independent on the loss and optimizer.
            model = keras.models.Sequential()
            kw = KWinners(percent_on=0.333,
                          k_inference_factor=1.5,
                          boost_strength=1.0,
                          boost_strength_factor=0.5,
                          duty_cycle_period=1000,
                          )
            model.add(kw)
            model.compile(
                loss="mse",
                optimizer=gradient_descent.GradientDescentOptimizer(0.01),
                run_eagerly=testing_utils.should_run_eagerly())

            # Ensure there are zero trainable parameters.
            layer = model.layers[0]
            trainable_weights = layer.trainable_weights
            self.assertEquals(0, len(trainable_weights))

            # Validate model prediction (i.e. a forward pass in testing mode).
            result = model.predict_on_batch(x)
            self.assertAllEqual(result, expected)

            # Ensure result doesn't change when only in testing mode.
            result = model.predict_on_batch(x)
            self.assertAllEqual(result, expected)

            # Validate one forward pass in training mode.
            y = np.zeros(x.shape, dtype=np.float32)
            # Expect 2 winners per batch (33% of 6)
            y[0, 0] = x[0, 0]
            y[0, 3] = x[0, 3]
            y[1, 2] = x[1, 2]
            y[1, 3] = x[1, 3]

            loss = model.train_on_batch(x, y)
            self.assertEquals(loss, 0.0)

            # Test values of updated duty cycle.
            old_duty = layer.duty_cycles
            old_duty = old_duty.numpy() if tf.executing_eagerly() else old_duty.eval()
            new_duty = np.array([1.0, 0, 1.0, 2.0, 0, 0], dtype=np.float32) / 2.0
            self.assertAllEqual(old_duty, new_duty)

            # Test forward with updated duty cycle.
            y = np.zeros(x.shape, dtype=np.float32)
            y[0, 1] = x[0, 1]
            y[0, 5] = x[0, 5]
            y[1, 1] = x[1, 1]
            y[1, 5] = x[1, 5]

            loss = model.train_on_batch(x, y)
            self.assertEqual(loss, 0.0)

    @keras_parameterized.run_all_keras_modes
    def test_three(self):
        """
        Test a series of calls on the layer in training mode.
        """

        x = self.x2
        y = x.copy()

        # Test layer within Sequential model.
        with self.cached_session():
            # Compile model. Results should be independent on the loss and optimizer.
            model = keras.models.Sequential()
            kw = KWinners(
                percent_on=0.333,
                k_inference_factor=1.5,
                boost_strength=1.0,
                boost_strength_factor=0.5,
                duty_cycle_period=1000)
            model.add(kw)
            model.compile(
                loss="mse",
                optimizer=gradient_descent.GradientDescentOptimizer(0.01),
                run_eagerly=testing_utils.should_run_eagerly())

            # Ensure there are zero trainable parameters.
            layer = model.layers[0]
            trainable_weights = layer.trainable_weights
            self.assertEquals(0, len(trainable_weights))

            # "Train" on a sequence of batches. This will only effect the duty cycle.
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)

            # expected = np.zeros_like(x)
            # expected[0, 0] = x[0, 0]
            # expected[0, 5] = x[0, 5]
            # expected[1, 2] = x[1, 2]
            # expected[1, 3] = x[1, 3]
            # result = model(x, training=True)
            # self.assertAllEqual(result, expected)

            # Validate model prediction (i.e. a forward pass in testing mode).
            result = model.predict_on_batch(x)
            expected = np.zeros_like(x)
            expected[0, 0] = x[0, 0]
            expected[0, 1] = x[0, 1]
            expected[0, 5] = x[0, 5]
            expected[1, 2] = x[1, 2]
            expected[1, 3] = x[1, 3]
            expected[1, 4] = x[1, 4]
            self.assertAllEqual(result, expected)


class KWinners2DLayerTest(keras_parameterized.TestCase):
    """
    Module for testing the 1D KWinners layer.
    """

    def setUp(self):
        super().setUp()

        # Batch size 1
        x = np.random.random((1, 3, 2, 2)).astype(np.float32) / 2.0
        x[0, 0, 1, 0] = 1.10
        x[0, 0, 1, 1] = 1.20
        x[0, 1, 0, 1] = 1.21
        x[0, 2, 1, 0] = 1.30
        self.x1 = x

        # Batch size 2
        x = np.random.random((2, 3, 2, 2)).astype(np.float32) / 2.0

        x[0, 0, 1, 0] = 1.10
        x[0, 0, 1, 1] = 1.20
        x[0, 1, 0, 1] = 1.21
        x[0, 2, 1, 0] = 1.30

        x[1, 0, 0, 0] = 1.40
        x[1, 1, 0, 0] = 1.50
        x[1, 1, 0, 1] = 1.60
        x[1, 2, 1, 1] = 1.70
        self.x2 = x

    @keras_parameterized.run_all_keras_modes
    def test_one(self):
        """Equal duty cycle, boost strength 0, k=4, batch size 1."""
        x = self.x1
        expected = np.zeros_like(x)
        expected[0, 0, 1, 0] = x[0, 0, 1, 0]
        expected[0, 0, 1, 1] = x[0, 0, 1, 1]
        expected[0, 1, 0, 1] = x[0, 1, 0, 1]
        expected[0, 2, 1, 0] = x[0, 2, 1, 0]

        n = np.prod(x.shape[1:])
        kwargs = {
            "percent_on": 4 / n,
            "k_inference_factor": 1.0,
            "boost_strength": 0.0,
            "data_format": "channels_first",
        }
        with self.cached_session(), keras.utils.custom_object_scope(CUSTOM_OBJECTS):
            testing_utils.layer_test(KWinners2d,
                                     kwargs=kwargs,
                                     input_data=x,
                                     expected_output=expected)

    @keras_parameterized.run_all_keras_modes
    def test_two(self):
        """Equal duty cycle, boost strength 0, k=3."""
        x = self.x1
        expected = np.zeros(x.shape)
        expected[0, 0, 1, 1] = x[0, 0, 1, 1]
        expected[0, 1, 0, 1] = x[0, 1, 0, 1]
        expected[0, 2, 1, 0] = x[0, 2, 1, 0]
        n = np.prod(x.shape[1:])
        kwargs = {
            "percent_on": 3 / n,
            "k_inference_factor": 1.0,
            "boost_strength": 0.0,
            "data_format": "channels_first",
        }

        with self.cached_session(), keras.utils.custom_object_scope(CUSTOM_OBJECTS):
            testing_utils.layer_test(KWinners2d,
                                     kwargs=kwargs,
                                     input_data=x,
                                     expected_output=expected)

    @keras_parameterized.run_all_keras_modes
    def test_three(self):
        """Equal duty cycle, boost strength=0, k=4, batch size=2."""
        x = self.x2
        expected = np.zeros(x.shape)
        expected[0, 0, 1, 0] = x[0, 0, 1, 0]
        expected[0, 0, 1, 1] = x[0, 0, 1, 1]
        expected[0, 1, 0, 1] = x[0, 1, 0, 1]
        expected[0, 2, 1, 0] = x[0, 2, 1, 0]
        expected[1, 0, 0, 0] = x[1, 0, 0, 0]
        expected[1, 1, 0, 0] = x[1, 1, 0, 0]
        expected[1, 1, 0, 1] = x[1, 1, 0, 1]
        expected[1, 2, 1, 1] = x[1, 2, 1, 1]

        n = np.prod(x.shape[1:])
        kwargs = {
            "percent_on": 4 / n,
            "k_inference_factor": 1.0,
            "boost_strength": 0.0,
            "data_format": "channels_first",
        }
        with self.cached_session(), keras.utils.custom_object_scope(CUSTOM_OBJECTS):
            testing_utils.layer_test(KWinners2d,
                                     kwargs=kwargs,
                                     input_data=x,
                                     expected_output=expected)

    @keras_parameterized.run_all_keras_modes
    def test_four(self):
        """Equal duty cycle, boost strength=0, k=3, batch size=2."""
        x = self.x2
        expected = np.zeros(x.shape)
        expected[0, 0, 1, 1] = x[0, 0, 1, 1]
        expected[0, 1, 0, 1] = x[0, 1, 0, 1]
        expected[0, 2, 1, 0] = x[0, 2, 1, 0]
        expected[1, 1, 0, 0] = x[1, 1, 0, 0]
        expected[1, 1, 0, 1] = x[1, 1, 0, 1]
        expected[1, 2, 1, 1] = x[1, 2, 1, 1]

        n = np.prod(x.shape[1:])
        kwargs = {
            "percent_on": 3 / n,
            "k_inference_factor": 1.0,
            "boost_strength": 0.0,
            "data_format": "channels_first",
        }
        with self.cached_session(), keras.utils.custom_object_scope(CUSTOM_OBJECTS):
            testing_utils.layer_test(KWinners2d,
                                     kwargs=kwargs,
                                     input_data=x,
                                     expected_output=expected)

    @keras_parameterized.run_all_keras_modes
    def test_five(self):
        """
        Test a series of calls on the layer in training mode.
        """

        x = self.x2
        y = x.copy()

        kwargs = {
            "percent_on": 0.25,
            "k_inference_factor": 0.5,
            "boost_strength": 1.0,
            "boost_strength_factor": 0.5,
            "duty_cycle_period": 1000,
            "data_format": "channels_first",
        }

        # Test layer within Sequential model.
        with self.cached_session():
            # Compile model. Results should be independent on the loss and optimizer.
            model = keras.models.Sequential()
            kw = KWinners2d(**kwargs)
            model.add(kw)
            model.compile(
                loss="mse",
                optimizer=gradient_descent.GradientDescentOptimizer(0.01),
                run_eagerly=testing_utils.should_run_eagerly())

            # Ensure there are zero trainable parameters.
            layer = model.layers[0]
            trainable_weights = layer.trainable_weights
            self.assertEquals(0, len(trainable_weights))

            # "Train" on a sequence of batches. This will only effect the duty cycle.
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)
            model.train_on_batch(x, y)

            # Validate model prediction (i.e. a forward pass in testing mode).
            expected = np.zeros_like(x)
            expected[0, 0, 1, 1] = x[0, 0, 1, 1]
            expected[0, 2, 1, 0] = x[0, 2, 1, 0]
            expected[1, 1, 0, 1] = x[1, 1, 0, 1]
            expected[1, 2, 1, 1] = x[1, 2, 1, 1]
            result = model.predict_on_batch(x)
            self.assertAllEqual(result, expected)


if __name__ == "__main__":
    test.main()
